{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dJ8dtokC57c8"
      },
      "outputs": [],
      "source": [
        "# !pip install datasets\n",
        "# !pip install transformers\n",
        "# !pip install transformers[torch]\n",
        "# !pip install accelerate -U"
      ],
      "id": "dJ8dtokC57c8"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import Libraries\n"
      ],
      "metadata": {
        "id": "a52ZZLdqiqMU"
      },
      "id": "a52ZZLdqiqMU"
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the necessary libraries for data processing, model training, and evaluation.\n",
        "%matplotlib inline\n",
        "import json, sys, os\n",
        "import numpy as np, pandas as pd\n",
        "import math\n",
        "import torch, torchaudio\n",
        "import random\n",
        "\n",
        "# Import the necessary typing modules for type hints.\n",
        "from typing import List, Dict, Union, Any\n",
        "\n",
        "# Import specific classes from the 'datasets' library to load and handle audio datasets.\n",
        "from datasets import load_dataset, Audio, Dataset\n",
        "\n",
        "# Import specific classes from the 'transformers' library for audio classification.\n",
        "from transformers import (\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    AutoFeatureExtractor,\n",
        "    AutoModelForAudioClassification,\n",
        "    AutoConfig,\n",
        "    set_seed\n",
        ")\n",
        "\n",
        "MY_PATH = 'gdsc' #the folder path with the data\n",
        "\n",
        "# Add the path f'{MY_PATH}/src' to the system path to access the 'gdsc_eval' module for evaluation purposes.\n",
        "sys.path.append(f'{MY_PATH}/src')\n",
        "from gdsc_eval import compute_metrics\n"
      ],
      "metadata": {
        "id": "HBu9Up6hSH71"
      },
      "id": "HBu9Up6hSH71",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Functions"
      ],
      "metadata": {
        "id": "Pz8BZuyFCXA6"
      },
      "id": "Pz8BZuyFCXA6"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocessing functions"
      ],
      "metadata": {
        "id": "xZmC43gSGbjs"
      },
      "id": "xZmC43gSGbjs"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9fd2fa9f-72b4-4bba-b514-0f6b173bc1fc"
      },
      "outputs": [],
      "source": [
        "random.seed(42) #for model reproducibility\n",
        "\n",
        "MAX_DURATION = 11 #max duration in  of the audio files (generally to pass to the feature extractor - it speed up the pre-processing step)\n",
        "\n",
        "def preprocess_function(examples: Dict[str, Any], path: bool = 1) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Preprocesses audio data for audio classification task.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "        examples: dict\n",
        "                  A dictionary containing the input examples, where the 'audio' key corresponds to the audio data.\n",
        "                  Each audio example should have a 'path' and 'array' field.\n",
        "        path: int (optional)\n",
        "                   An integer flag indicating whether to include the 'file_name' field in the output.\n",
        "                   Default is 1, which includes the 'file_name' field. Set to 0 to exclude it.\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "        dict: A dictionary containing the preprocessed inputs for audio classification.\n",
        "              The returned dictionary includes the following fields:\n",
        "              - 'input_values': The audio arrays preprocessed by the feature extractor, truncated to MAX_DURATION seconds.\n",
        "              - 'label' (optional): The true labels of audio arrays.\n",
        "              - 'attention_mask' (optional): If 'return_attention_mask' is True in the feature extractor, this field will be present.\n",
        "              - 'file_name' (optional): If 'path' is set to 1, this field contains the filenames extracted from the 'path' field of input examples.\n",
        "    \"\"\"\n",
        "\n",
        "    # Extract audio arrays from the input examples and truncate them to MAX_DURATION seconds.\n",
        "    audio_arrays = [x[\"array\"][:MODEL_SAMPLING_RATE*MAX_DURATION] for x in examples['audio']]\n",
        "\n",
        "    # Use the feature extractor to preprocess the audio data.\n",
        "    inputs = feature_extractor(\n",
        "        audio_arrays,\n",
        "        sampling_rate=feature_extractor.sampling_rate,\n",
        "        truncation=True,\n",
        "        return_attention_mask=False,\n",
        "    )\n",
        "\n",
        "    # Include 'file_name' field in the output if 'path' is set to 1.\n",
        "    if path:\n",
        "        inputs['file_name'] = [e['path'].split('/')[-1] for e in examples['audio']]\n",
        "\n",
        "    return inputs\n",
        "\n",
        "\n",
        "\n",
        "def chunk_aug(example: np.ndarray) -> np.ndarray:\n",
        "\n",
        "    \"\"\"\n",
        "    Randomly selects a chunk from the input audio signal and returns the chunk with a maximum duration of MAX_DURATION seconds.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    example: numpy.ndarray\n",
        "          The input audio signal represented as a 1-D numpy array.\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "        numpy.ndarray: A 1-D numpy array representing the selected chunk from the input audio signal 'example'. The maximum duration of the chunk is MAX_DURATION seconds, but it may be shorter if 'example' is not long enough.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    e_len = int(example.shape[0]/MODEL_SAMPLING_RATE)  #length of audio\n",
        "    min_len = min([2, e_len]) #min possible seconds\n",
        "    max_len = min([MAX_DURATION, e_len]) #max possible seconds\n",
        "    chunk_len = list(range(min_len, max_len+1)) #how many seconds\n",
        "    chunk_len_ran = random.choice(chunk_len) #random chunk seconds\n",
        "\n",
        "    e_len_range = list(range(0,e_len-chunk_len_ran+1)) #positions\n",
        "    e_len_range_ran = random.choice(e_len_range) #random position\n",
        "    example = example[e_len_range_ran*MODEL_SAMPLING_RATE:(e_len_range_ran+chunk_len_ran)*MODEL_SAMPLING_RATE] #get random chunk from audio\n",
        "\n",
        "    return example[:MODEL_SAMPLING_RATE*MAX_DURATION]\n",
        "\n",
        "\n",
        "def call_files(x: Dict[str, Union[List[int], torch.Tensor]]) -> Dict[str, Any]:\n",
        "\n",
        "    \"\"\"\n",
        "    Loads audio files based on the labels in 'x', applies chunk augmentation, extracts features using a feature extractor,\n",
        "    and returns the processed inputs.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    x : dict\n",
        "        A dictionary containing the input data.\n",
        "        Required keys:\n",
        "            - 'label': A list of integers representing the labels.\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    dict\n",
        "        A dictionary containing the processed inputs.\n",
        "        The dictionary has the following keys:\n",
        "            - 'input_values': A torch.Tensor representing the processed audio inputs.\n",
        "            - 'label': A list of integers representing the labels.\n",
        "    \"\"\"\n",
        "    # Select a random file path for each label in 'x' from 'balanced_df_list'.\n",
        "    path_files = [random.choice(balanced_df_list[l]) for l in x['label']]\n",
        "\n",
        "    # Extract the file names from the selected file paths.\n",
        "    file_name = [p.split('/')[-1] for p in path_files]\n",
        "\n",
        "    # Apply chunk augmentation to the audio signals and store the augmented chunks in 'wv'.\n",
        "    wv = [chunk_aug(np.array(torchaudio.load(p)[0][0].numpy())) for p in path_files]\n",
        "\n",
        "    # Extract features from the augmented audio signals using the feature extractor.\n",
        "    inputs = feature_extractor(\n",
        "        wv,\n",
        "        sampling_rate=feature_extractor.sampling_rate,\n",
        "        truncation=True,\n",
        "        return_attention_mask=False)\n",
        "\n",
        "    # Convert the 'input_values' to a torch.Tensor and store it in the 'inputs' dictionary.\n",
        "    inputs['input_values'] = torch.Tensor(np.array(inputs['input_values']))\n",
        "\n",
        "    # Store the 'label' values from the input dictionary 'x' in the 'inputs' dictionary.\n",
        "    inputs['label'] = x['label']\n",
        "\n",
        "    return inputs\n"
      ],
      "id": "9fd2fa9f-72b4-4bba-b514-0f6b173bc1fc"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Predicting functions\n",
        "\n",
        "This function divides the input audio signal 'example' into multiple chunks of 11 seconds (the standard AST model takes ~11 second of a waveform and transforms it into a mel spectogram of 1024 frames) each for prediction, ensuring that each chunk contains a maximum of 11 seconds of audio. If the length of 'example' is less than 11 seconds, the function returns the original 'example' as a single chunk.\n",
        "\n",
        "The function performs the following steps:\n",
        "1. Calculate the length of the audio signal 'example' in seconds ('e_len') based on the 'MODEL_SAMPLING_RATE'.\n",
        "2. If 'e_len' is greater than 11 seconds, calculate the minimum possible length ('min_len') for the chunks, considering the remaining audio after the last 11-second chunk.\n",
        "3. Create a list of chunk lengths ('chunk_len') ranging from 0 to 'min_len' (exclusive) with a step of 2 seconds.\n",
        "4. Split the input audio signal 'e' into multiple chunks, each starting at different 2 seconds position and having a duration of max 11 seconds.\n",
        "5. Return the list of chunks.\n",
        "\n",
        "Example:\n",
        "--------\n",
        "Given an audio signal 'example' with a length of 15 seconds, the function will return 3 chunks:\n",
        "- Chunk 1: Audio from 0 to 11 seconds (~11 seconds)\n",
        "- Chunk 2: Audio from 2 to 13 seconds (~11 seconds)\n",
        "- Chunk 3: Audio from 4 to 15 seconds (~11 seconds)"
      ],
      "metadata": {
        "id": "rzeV8jS6GtGQ"
      },
      "id": "rzeV8jS6GtGQ"
    },
    {
      "cell_type": "code",
      "source": [
        "def chunk_pred(example: np.ndarray) -> List[np.ndarray]:\n",
        "    \"\"\"\n",
        "    Divide the input audio signal 'example' into multiple chunks and return a list of these chunks for prediction.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    example : numpy.ndarray\n",
        "        The input audio signal represented as a 1-D numpy array.\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    List[numpy.ndarray]\n",
        "        A list of numpy arrays, each representing a chunk of the input audio signal for prediction.\n",
        "\n",
        "    \"\"\"\n",
        "    e_len = int(example.shape[0]/MODEL_SAMPLING_RATE)\n",
        "    if e_len > MAX_DURATION: #if length of audio is more than MAX_DURATION seconds, divide the audio to chunks\n",
        "        min_len = min(360, e_len-11) #min possible seconds\n",
        "        chunk_len = list(range(0, min_len, 2)) #how many seconds\n",
        "        return [example[MODEL_SAMPLING_RATE*r:MODEL_SAMPLING_RATE*(MAX_DURATION+r)] for r in chunk_len]\n",
        "    return [example[:MODEL_SAMPLING_RATE*MAX_DURATION]]"
      ],
      "metadata": {
        "id": "XH_HOfAkG6ST"
      },
      "id": "XH_HOfAkG6ST",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This next function preprocesses the audio data from the 'examples' dictionary in chunks using the 'chunk_pred' function.\n",
        "It then extracts features from the chunks using the provided feature extractor.\n",
        "The 'max_duration', 'feature_extractor', and 'MODEL_SAMPLING_RATE'.\n",
        "\n",
        "The function performs the following steps:\n",
        "1. Calls the 'chunk_pred' function to chunk the audio data from the 'examples' dictionary.\n",
        "2. Calls the feature extractor with the chunked audio data to extract features.\n",
        "3. Moves the 'model' to the 'cuda:0' device for GPU acceleration.\n",
        "4. Converts the input values to a torch.Tensor and moves it to the 'cuda:0' device.\n",
        "5. Performs predictions for each chunk using the 'model' and stores the logits.\n",
        "6. Extracts the predicted class IDs and their corresponding prediction scores.\n",
        "7. Finds the class ID with the highest prediction score for the entire example.\n",
        "8. Updates the 'examples' dictionary with the prediction information and the file name of the audio example.\n",
        "9. Returns the updated 'examples' dictionary with prediction information."
      ],
      "metadata": {
        "id": "Ycw1CS49H6j4"
      },
      "id": "Ycw1CS49H6j4"
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_function_pred_chunks(examples: Dict[str, Any], model: torch.nn.Module) -> Dict[str, Any]:\n",
        "\n",
        "    \"\"\"\n",
        "    Preprocesses audio examples in chunks for prediction using the provided model.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    examples : dict\n",
        "        A dictionary containing the audio examples.\n",
        "        The 'audio' key holds another dictionary with two keys: 'array' (numpy.ndarray) and 'path' (str).\n",
        "    model : torch.nn.Module\n",
        "        The audio classification model used for prediction.\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    dict\n",
        "        A dictionary containing the processed audio examples with additional prediction information.\n",
        "        The keys in the dictionary include:\n",
        "            - 'audio': A dictionary with the processed audio data.\n",
        "            - 'file_name': A string representing the file name of the audio example.\n",
        "            - 'pred_id': A list of tuples containing the class ID and its corresponding prediction score for each chunk.\n",
        "            - 'predicted_class_id': An integer representing the predicted class ID for the example.\n",
        "    \"\"\"\n",
        "\n",
        "    # Chunk the audio data using the 'chunk_pred' function\n",
        "    wv = chunk_pred(examples['audio']['array'])\n",
        "\n",
        "    # Extract features from the chunked audio data using the feature extractor\n",
        "    inputs = feature_extractor(\n",
        "        wv,\n",
        "        sampling_rate=feature_extractor.sampling_rate,\n",
        "        truncation=True,\n",
        "        return_attention_mask=False,\n",
        "    )\n",
        "\n",
        "    # Move the model to the 'cuda:0' device for GPU acceleration\n",
        "    model_pred = model.to('cuda:0')\n",
        "\n",
        "    # Convert the input values to a torch.Tensor and move it to the 'cuda:0' device\n",
        "    input_values = torch.Tensor(np.array(inputs['input_values'])).to('cuda:0')\n",
        "\n",
        "    # Perform predictions for each chunk using the model and store the logits\n",
        "    logits = []\n",
        "    with torch.no_grad():\n",
        "        logits = [model(i.unsqueeze(0)).logits.cpu() for i in input_values]\n",
        "\n",
        "    # Concatenate the logits and extract the predicted class IDs and their corresponding prediction scores\n",
        "    logits = torch.Tensor(np.concatenate(logits))\n",
        "    predicted_class_id_max = [torch.max(item).item() for item in logits]\n",
        "    predicted_class_id = [(int(torch.argmax(item).item()), torch.max(item).item()) for item in logits]\n",
        "\n",
        "    # Find the class ID with the highest prediction score for the entire example\n",
        "    pred_id_max = torch.Tensor(predicted_class_id)[:, 1:].argmax().item()\n",
        "\n",
        "    # Update the 'examples' dictionary with prediction information and file name\n",
        "    examples['pred_id'] = predicted_class_id\n",
        "    examples['predicted_class_id'] = predicted_class_id[pred_id_max][0]\n",
        "    examples['file_name'] = examples['audio']['path'].split('/')[-1]\n",
        "\n",
        "    # Return the updated 'examples' dictionary with prediction information\n",
        "    return examples"
      ],
      "metadata": {
        "id": "NJT5PNt4IKt-"
      },
      "id": "NJT5PNt4IKt-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing the data"
      ],
      "metadata": {
        "id": "2dRdIXDtjCgC"
      },
      "id": "2dRdIXDtjCgC"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Mapping Class Labels to Numerical IDs for Audio Dataset\n",
        "\n",
        "(*NOTE: the train dataset is prepared differently and it will be explained in the next paragraph*)\n",
        "\n",
        "1.   Defining file paths for the validation and test datasets\n",
        "2.   Loading class labels from a JSON file, and creates two dictionaries to map class labels to their corresponding numerical IDs and vice versa.\n",
        "\n",
        "The resulting dictionaries enable convenient encoding and decoding of class labels in the audio dataset, making it suitable for further processing and model training."
      ],
      "metadata": {
        "id": "y0YczGyDAgoX"
      },
      "id": "y0YczGyDAgoX"
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the file paths for the validation and test datasets.\n",
        "val_path = f'{MY_PATH}/data/val'\n",
        "test_path = f'{MY_PATH}/data/test'\n",
        "\n",
        "# Open the file containing the labels in JSON format and load its content into the 'labels' dictionary.\n",
        "with open(f'{MY_PATH}/data/labels.json', 'r') as f:\n",
        "    labels = json.load(f)\n",
        "\n",
        "# Initialize two dictionaries 'label2id' and 'id2label' to map the class labels to their corresponding numerical IDs and vice versa.\n",
        "label2id, id2label = dict(), dict()\n",
        "\n",
        "# Iterate through the 'labels' dictionary and populate the 'label2id' and 'id2label' dictionaries.\n",
        "for k, v in labels.items():\n",
        "    label2id[k] = str(v)  # Map class label 'k' to its numerical ID 'v' as a string.\n",
        "    id2label[str(v)] = k  # Map numerical ID 'v' as a string back to its corresponding class label 'k'.\n",
        "\n",
        "# Calculate the number of unique labels in the dataset and store it in 'num_labels'.\n",
        "num_labels = len(label2id)\n"
      ],
      "metadata": {
        "id": "JhQ5JgxVSb1u"
      },
      "id": "JhQ5JgxVSb1u",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Creating an automatic Feature Extractor Using a Pre-trained Model\n",
        "\n",
        "***Note:*** We are using `AutoFeatureExtractor` (instead of ASTFeatureExtractor) which allows us to be flexible in terms of choosing different type of feature extractors for different type of model architectures."
      ],
      "metadata": {
        "id": "JsLmHyJEBEGA"
      },
      "id": "JsLmHyJEBEGA"
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the pre-trained model to be used for audio classification.\n",
        "THE_MODEL = \"MIT/ast-finetuned-audioset-10-10-0.4593\"\n",
        "\n",
        "# Define the sampling rate of the audio data to be processed by the feature extractor.\n",
        "MODEL_SAMPLING_RATE = 44100\n",
        "\n",
        "# Create a feature extractor object ('feature_extractor') by loading the pre-trained model specified by 'THE_MODEL'. The feature extractor is responsible for converting audio data into suitable features for the model.\n",
        "feature_extractor = AutoFeatureExtractor.from_pretrained(\n",
        "    THE_MODEL,\n",
        "    do_normalize=True,                    # enable normalization of the audio data\n",
        "    return_attention_mask=False,          # 'return_attention_mask=False' disables returning attention masks.\n",
        "    sampling_rate=MODEL_SAMPLING_RATE,    # sets the sampling rate\n",
        "    num_mel_bins=128                      # specifying the number of Mel bins used for the Mel spectrogram feature representation.\n",
        ")\n"
      ],
      "metadata": {
        "id": "LvqcyU9rSuX0"
      },
      "id": "LvqcyU9rSuX0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating Hugging Face Dataset Objects for Audio Classification\n",
        "\n"
      ],
      "metadata": {
        "id": "nGXeLJqnB6zk"
      },
      "id": "nGXeLJqnB6zk"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "481adc79-ba97-4c80-b891-4c928eb35f4e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 200,
          "referenced_widgets": [
            "cceb7a9586c54edea23246dff1932b8a",
            "1269afec90064d03bffc7cf1b40dda18",
            "14875bb04d29487ea48e71ba56673faf",
            "2644de80f7ac4e4c98b80dc95479698e",
            "055760bd2f864fcfb2a3eccefabcbdf1",
            "40eceae3e88b4d36afa05bfb775f3f5b",
            "e4fdf479f078460bba0eacf0ce7d2bc5",
            "bd699fe22f934748a5acdde2ab1bf052",
            "855457e393db4775ab34e43f81ff44b2",
            "e2f2e11be2e64b1d9246fe5af5ffafab",
            "12f74ed3c7554df3b7e142bb58425f59",
            "a7011dac3cb6477596042bcb0159ec4d",
            "1559ac70674c45a3bd9dcf2dc75921a4",
            "7f64587706cd4bffa582eeb5b883d494",
            "2b5e606a3f5c4598af65ffdb5446daf1",
            "87b05c966f6c49f19c800be329553686",
            "c3177d4decda4a16ba4a19be2b3c0f37",
            "c20183e0dc394e049c5a3c68f2f19256",
            "5b93f4bb24a34b1aaa54782f03116fd9",
            "8612c99f887b4e5da40c2bfa3da2bdd4",
            "7778b27815e8496cac084ee06489300d",
            "2e9758759ade45a8932c7b40e2952940",
            "9e7a5810af29401c8480d38e719d616a",
            "cd0515231c9648f1b660f749fbd29a9d",
            "62a08b0f68cd4086bc11772435e241a2",
            "b15496c9085a4b689b18077b5fa5b17a",
            "6db9e0697f944606b1db70dcdb2fc837",
            "9cac9917bc0f4ce19f01d1fb6aad735c",
            "cfdda946c5924a509531d4910fb8b4a3",
            "602a038ab8264406aab070d22b72f424",
            "e9046a3c8e94468d93fbc6255b33f9b0",
            "6bcd0c51353f440f9820cfaab3af85a4",
            "c778835e15ff4b68a1fb2501582c6d3d",
            "5e42a61b3ec743fda502cf7df9960819",
            "c7c26944a14244e9a5f7477c55217e56",
            "ad565dc2a7a34466a9253e1e4d1bf85b",
            "cf57ccbdc65643368f40d580c853a232",
            "3efc2571022c4798b1766bac5ecb86d9",
            "a4533dfdf1954a9d940016adab5c316e",
            "9eee968399b9454d9327c052b35318dc",
            "fdca30d835fd4215b527cb0705643dd4",
            "0969db5fec97459c9e2e931f48a5864d",
            "ac75da7f3c0b41fda42ba5c92b8b2cac",
            "52060d9c48914dff80dff532685f0317"
          ]
        },
        "outputId": "8cadf719-a928-4e97-d3a2-5998ccd3a29f"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Resolving data files:   0%|          | 0/580 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cceb7a9586c54edea23246dff1932b8a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:datasets.builder:Found cached dataset audiofolder (/root/.cache/huggingface/datasets/audiofolder/default-fd10af1771044605/0.0.0/6cbdd16f8688354c63b4e2a36e1585d05de285023ee6443ffd71c4182055c0fc)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/1 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a7011dac3cb6477596042bcb0159ec4d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Resolving data files:   0%|          | 0/557 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9e7a5810af29401c8480d38e719d616a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:datasets.builder:Found cached dataset audiofolder (/root/.cache/huggingface/datasets/audiofolder/default-38aa57a483446631/0.0.0/6cbdd16f8688354c63b4e2a36e1585d05de285023ee6443ffd71c4182055c0fc)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/1 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5e42a61b3ec743fda502cf7df9960819"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Load the validation dataset from the \"audiofolder\" format using the 'load_dataset' function.\n",
        "# The 'val_path' variable represents the path to the directory containing the validation data.\n",
        "val_dataset = load_dataset(\"audiofolder\", data_dir=val_path).get('train')\n",
        "\n",
        "# Load the test dataset from the \"audiofolder\" format using the 'load_dataset' function.\n",
        "# The 'test_path' variable represents the path to the directory containing the test data.\n",
        "test_dataset = load_dataset(\"audiofolder\", data_dir=test_path).get('train')\n",
        "\n",
        "# Convert the 'audio' column in the validation and test dataset to the 'Audio' data type with a specific 'sampling_rate'.\n",
        "val_dataset = val_dataset.cast_column(\"audio\", Audio(sampling_rate=MODEL_SAMPLING_RATE))\n",
        "test_dataset = test_dataset.cast_column(\"audio\", Audio(sampling_rate=MODEL_SAMPLING_RATE))\n"
      ],
      "id": "481adc79-ba97-4c80-b891-4c928eb35f4e"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Encode the validation and test dataset by applying the `preprocess_function` to each example."
      ],
      "metadata": {
        "id": "pIwcxKJCDpHi"
      },
      "id": "pIwcxKJCDpHi"
    },
    {
      "cell_type": "code",
      "source": [
        "# The 'preprocess_function' is applied with the argument 'path=0', indicating that function will not return 'file_name' column.\n",
        "# The dataset is processed in batches of size 2 for efficiency ('batch_size=2').\n",
        "val_dataset_encoded = val_dataset.map(\n",
        "    lambda x: preprocess_function(x, path=0), remove_columns=[\"audio\"], batched=True, batch_size=2)\n",
        "\n",
        "# Set the format of the validation dataset to use Torch tensors ('type='torch'') and include all columns in the output ('output_all_columns=True').\n",
        "val_dataset_encoded.set_format(type='torch', columns=['input_values'], output_all_columns=True)\n",
        "\n",
        "\n",
        "\n",
        "# No specific 'path' argument is provided, so 'path' argument is 1(True) by default. It means function will return 'file_name' column.\n",
        "# The dataset is processed in batches of size 2 for efficiency ('batch_size=2').\n",
        "test_dataset_encoded = test_dataset.map(\n",
        "    preprocess_function, remove_columns=[\"audio\"], batched=True, batch_size=2)\n",
        "\n",
        "# Set the format of the test dataset to use Torch tensors ('type='torch'') and include all columns in the output ('output_all_columns=True').\n",
        "test_dataset_encoded.set_format(type='torch', columns=['input_values'], output_all_columns=True)\n",
        "\n",
        "val_dataset_encoded, test_dataset_encoded"
      ],
      "metadata": {
        "id": "68CJ406JMk3a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ee1a301-b42e-4886-d6e1-53d5cd63d421"
      },
      "id": "68CJ406JMk3a",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/audiofolder/default-fd10af1771044605/0.0.0/6cbdd16f8688354c63b4e2a36e1585d05de285023ee6443ffd71c4182055c0fc/cache-01c082fb6d406b9b.arrow\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/audiofolder/default-38aa57a483446631/0.0.0/6cbdd16f8688354c63b4e2a36e1585d05de285023ee6443ffd71c4182055c0fc/cache-03c372436a802242.arrow\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(Dataset({\n",
              "     features: ['label', 'input_values'],\n",
              "     num_rows: 579\n",
              " }),\n",
              " Dataset({\n",
              "     features: ['input_values', 'file_name'],\n",
              "     num_rows: 556\n",
              " }))"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating a Balanced Dataset\n",
        "\n",
        "*Note: Due to more characteristics/patterns in validation test provided, we decided to train the model on all of the data train+validation provided.*\n",
        "\n",
        "1. The dataset is constructed with 1320 examples, ensuring each of the 66 labels appears 20 times.\n",
        "2. The 'audio' column in the dataset is initialized with empty strings, and the 'label' column is populated with integers from 0 to 65, repeating each label 20 times.\n",
        "3. To enhance the empty dataset, metadata is loaded from a CSV file and concatenated with the corresponding file paths, forming the full paths to the audio files. This facilitates easy access to the data during model training.\n",
        "4. The file paths are then grouped based on their labels, creating a DataFrame named 'balanced_df_list', which contains two columns: 'label' and 'path'.\n",
        "5. The 'balanced_dataset' object is created using the 'balanced_dataset_dict', representing the balanced dataset, and a transform function, 'call_files', is set to process the audio data which applies chunk augmentation, extracts features using a feature extractor for each chunk and returns the processed inputs.\n",
        "\n",
        "The 'output_all_columns=True' parameter ensures that all transformed columns are included in the output, making it ready for model training or evaluation."
      ],
      "metadata": {
        "id": "QUYhM_MmELvn"
      },
      "id": "QUYhM_MmELvn"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6d82871a-e95e-4b3e-b3b6-b1d87c98e529"
      },
      "outputs": [],
      "source": [
        "random.seed(42) #for model reproducibility\n",
        "\n",
        "# Create a dictionary 'balanced_dataset_dict' to represent a balanced dataset where each label appears 20 times, resulting in a total of 20 * 66 = 1320 examples.\n",
        "balanced_dataset_dict = {\n",
        "    'audio': ['']*20*66,\n",
        "    'label': list(range(66))*20,\n",
        "}\n",
        "\n",
        "# Create a Dataset object 'balanced_dataset' from the 'balanced_dataset_dict'.\n",
        "balanced_dataset = Dataset.from_dict(balanced_dataset_dict)\n",
        "\n",
        "# Read the metadata from the CSV file located at f'{MY_PATH}/data/metadata.csv' into a pandas DataFrame 'all_data'.\n",
        "all_data = pd.read_csv(f'{MY_PATH}/data/metadata.csv')\n",
        "\n",
        "# Concatenate the string f'{MY_PATH}/' with each value in the 'path' column of 'all_data'.\n",
        "all_data['path'] = f'{MY_PATH}/' + all_data['path']\n",
        "\n",
        "# Group the DataFrame 'all_data' by the 'label' column and aggregate the 'path' column values into lists for each label.\n",
        "balanced_df_list = all_data[['label','path']].groupby('label')['path'].apply(list)\n",
        "\n",
        "# Set the transform function for 'balanced_dataset' to 'call_files'. This function will be applied to each example in 'balanced_dataset' to process the audio data and extract features.\n",
        "balanced_dataset.set_transform(call_files, output_all_columns=True)"
      ],
      "id": "6d82871a-e95e-4b3e-b3b6-b1d87c98e529"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create a model configuration and an audio classification model"
      ],
      "metadata": {
        "id": "skB-CxSmE64g"
      },
      "id": "skB-CxSmE64g"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f74c636d-618d-4e88-a81f-450dd0209270",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "01970753-d3ea-4fbf-9171-843f83b8b7e4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at MIT/ast-finetuned-audioset-10-10-0.4593 were not used when initializing ASTForAudioClassification: ['audio_spectrogram_transformer.encoder.layer.8.attention.attention.key.weight', 'audio_spectrogram_transformer.encoder.layer.10.attention.attention.query.weight', 'audio_spectrogram_transformer.encoder.layer.10.intermediate.dense.bias', 'audio_spectrogram_transformer.encoder.layer.9.layernorm_after.weight', 'audio_spectrogram_transformer.encoder.layer.8.attention.attention.query.bias', 'audio_spectrogram_transformer.encoder.layer.11.output.dense.bias', 'audio_spectrogram_transformer.encoder.layer.8.layernorm_before.bias', 'audio_spectrogram_transformer.encoder.layer.9.intermediate.dense.weight', 'audio_spectrogram_transformer.encoder.layer.11.attention.attention.value.bias', 'audio_spectrogram_transformer.encoder.layer.9.layernorm_before.weight', 'audio_spectrogram_transformer.encoder.layer.10.layernorm_after.weight', 'audio_spectrogram_transformer.encoder.layer.8.layernorm_after.bias', 'audio_spectrogram_transformer.encoder.layer.11.output.dense.weight', 'audio_spectrogram_transformer.encoder.layer.9.output.dense.weight', 'audio_spectrogram_transformer.encoder.layer.10.attention.output.dense.bias', 'audio_spectrogram_transformer.encoder.layer.11.intermediate.dense.weight', 'audio_spectrogram_transformer.encoder.layer.8.layernorm_after.weight', 'audio_spectrogram_transformer.encoder.layer.11.attention.attention.query.bias', 'audio_spectrogram_transformer.encoder.layer.8.attention.attention.value.weight', 'audio_spectrogram_transformer.encoder.layer.11.attention.output.dense.weight', 'audio_spectrogram_transformer.encoder.layer.8.attention.attention.value.bias', 'audio_spectrogram_transformer.encoder.layer.10.intermediate.dense.weight', 'audio_spectrogram_transformer.encoder.layer.11.attention.attention.key.weight', 'audio_spectrogram_transformer.encoder.layer.10.attention.attention.query.bias', 'audio_spectrogram_transformer.encoder.layer.9.intermediate.dense.bias', 'audio_spectrogram_transformer.encoder.layer.9.attention.output.dense.bias', 'audio_spectrogram_transformer.encoder.layer.9.attention.attention.key.weight', 'audio_spectrogram_transformer.encoder.layer.9.layernorm_before.bias', 'audio_spectrogram_transformer.encoder.layer.10.attention.attention.key.bias', 'audio_spectrogram_transformer.encoder.layer.10.layernorm_before.weight', 'audio_spectrogram_transformer.encoder.layer.8.attention.attention.query.weight', 'audio_spectrogram_transformer.encoder.layer.9.attention.attention.key.bias', 'audio_spectrogram_transformer.encoder.layer.8.layernorm_before.weight', 'audio_spectrogram_transformer.encoder.layer.8.attention.output.dense.weight', 'audio_spectrogram_transformer.encoder.layer.9.attention.output.dense.weight', 'audio_spectrogram_transformer.encoder.layer.11.attention.attention.key.bias', 'audio_spectrogram_transformer.encoder.layer.8.output.dense.bias', 'audio_spectrogram_transformer.encoder.layer.11.attention.output.dense.bias', 'audio_spectrogram_transformer.encoder.layer.8.output.dense.weight', 'audio_spectrogram_transformer.encoder.layer.9.output.dense.bias', 'audio_spectrogram_transformer.encoder.layer.9.attention.attention.value.bias', 'audio_spectrogram_transformer.encoder.layer.11.layernorm_after.weight', 'audio_spectrogram_transformer.encoder.layer.11.attention.attention.query.weight', 'audio_spectrogram_transformer.encoder.layer.10.layernorm_before.bias', 'audio_spectrogram_transformer.encoder.layer.9.attention.attention.query.weight', 'audio_spectrogram_transformer.encoder.layer.11.attention.attention.value.weight', 'audio_spectrogram_transformer.encoder.layer.10.layernorm_after.bias', 'audio_spectrogram_transformer.encoder.layer.10.output.dense.weight', 'audio_spectrogram_transformer.encoder.layer.8.attention.attention.key.bias', 'audio_spectrogram_transformer.encoder.layer.10.attention.attention.key.weight', 'audio_spectrogram_transformer.encoder.layer.9.attention.attention.value.weight', 'audio_spectrogram_transformer.encoder.layer.8.intermediate.dense.bias', 'audio_spectrogram_transformer.encoder.layer.10.output.dense.bias', 'audio_spectrogram_transformer.encoder.layer.10.attention.output.dense.weight', 'audio_spectrogram_transformer.encoder.layer.9.attention.attention.query.bias', 'audio_spectrogram_transformer.encoder.layer.10.attention.attention.value.bias', 'audio_spectrogram_transformer.encoder.layer.11.layernorm_before.bias', 'audio_spectrogram_transformer.encoder.layer.9.layernorm_after.bias', 'audio_spectrogram_transformer.encoder.layer.11.layernorm_before.weight', 'audio_spectrogram_transformer.encoder.layer.8.intermediate.dense.weight', 'audio_spectrogram_transformer.encoder.layer.10.attention.attention.value.weight', 'audio_spectrogram_transformer.encoder.layer.11.intermediate.dense.bias', 'audio_spectrogram_transformer.encoder.layer.8.attention.output.dense.bias', 'audio_spectrogram_transformer.encoder.layer.11.layernorm_after.bias']\n",
            "- This IS expected if you are initializing ASTForAudioClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing ASTForAudioClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of ASTForAudioClassification were not initialized from the model checkpoint at MIT/ast-finetuned-audioset-10-10-0.4593 and are newly initialized because the shapes did not match:\n",
            "- classifier.dense.weight: found shape torch.Size([527, 768]) in the checkpoint and torch.Size([66, 768]) in the model instantiated\n",
            "- classifier.dense.bias: found shape torch.Size([527]) in the checkpoint and torch.Size([66]) in the model instantiated\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "set_seed(42) #for model reproducibility\n",
        "\n",
        "# Create a model configuration 'model_config' using the AutoConfig class, which is initialized with pretrained settings from 'THE_MODEL'.\n",
        "model_config = AutoConfig.from_pretrained(\n",
        "    THE_MODEL,\n",
        "    num_labels=num_labels,       #the number of output labels for the audio classification task.\n",
        "    label2id=label2id,\n",
        "    id2label=id2label,\n",
        "    num_hidden_layers=8,         #number of hidden layers\n",
        "    ignore_mismatched_sizes=True #parameter is set to True, enabling the model to handle inputs of different sizes during inference.\n",
        "    )\n",
        "\n",
        "\n",
        "# Create an audio classification model 'model' using the AutoModelForAudioClassification class.\n",
        "model = AutoModelForAudioClassification.from_pretrained(\n",
        "    THE_MODEL,\n",
        "    config=model_config,\n",
        "    ignore_mismatched_sizes=True #parameter is set to True, enabling the model to handle inputs of different sizes during inference.\n",
        "    )\n"
      ],
      "id": "f74c636d-618d-4e88-a81f-450dd0209270"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training and Evaluation Configuration for Audio Classification Model"
      ],
      "metadata": {
        "id": "H2KEBrsZFI9R"
      },
      "id": "H2KEBrsZFI9R"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f3f08a73-4f61-4d09-b355-72c03411cf4a"
      },
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments\n",
        "\n",
        "NUM_TRAIN_EPOCHS = 12                                                   # variable defining number of training epochs\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=f'{MY_PATH}/models/final',       # directory for saving model checkpoints and logs\n",
        "    num_train_epochs=NUM_TRAIN_EPOCHS,                                  # number of epochs\n",
        "    per_device_train_batch_size=4,                                      # number of examples in batch for training\n",
        "    per_device_eval_batch_size=4,                                       # number of examples in batch for evaluation\n",
        "    evaluation_strategy=\"epoch\",                                        # makes evaluation at the end of each epoch\n",
        "    learning_rate=float(3e-5),                                          # learning rate\n",
        "    optim=\"adamw_torch\",                                                # optimizer\n",
        "    logging_steps=1,                                                    # number of steps for logging the training process - one step is one batch\n",
        "    load_best_model_at_end=True,                                        # whether to load or not the best model at the end of the training\n",
        "    metric_for_best_model=\"eval_loss\",                                  # claiming that the best model is the one with the lowest loss on the validation set\n",
        "    save_strategy='epoch',                                              # saving is done at the end of each epoch\n",
        "    gradient_accumulation_steps=8,                                      # the number of gradient accumulation steps to be used during training.\n",
        "    remove_unused_columns=False,\n",
        ")\n",
        "\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,                                                        # passing our model\n",
        "    args=training_args,                                                 # passing the above created arguments\n",
        "    train_dataset=balanced_dataset,                                     # passing the balanced set\n",
        "    eval_dataset=val_dataset_encoded,                                   # passing the encoded validation set\n",
        "    tokenizer=feature_extractor,                                        # passing the feature extractor\n",
        "    compute_metrics=compute_metrics,                                    # passing the compute_metrics function that we imported from gdsc_eval module\n",
        ")\n"
      ],
      "id": "f3f08a73-4f61-4d09-b355-72c03411cf4a"
    },
    {
      "cell_type": "code",
      "source": [
        "# layers=8, grad=8, lr=3e-5, batch_size=4, num_mel_bins=128\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "nt6NqNq-Uaqt"
      },
      "id": "nt6NqNq-Uaqt",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prediction of test set"
      ],
      "metadata": {
        "id": "FrCfWMosHT5T"
      },
      "id": "FrCfWMosHT5T"
    },
    {
      "cell_type": "code",
      "source": [
        "# Construct the path to the checkpoint directory based on the checkpoint number 'N' of your preference.\n",
        "N = 495 #our best model\n",
        "checkpoint = f'{MY_PATH}/models/final/checkpoint-{N}'\n",
        "\n",
        "# Create a model configuration 'model_config_pred' using the 'AutoConfig' class and load it from the specified checkpoint directory.\n",
        "model_config_pred = AutoConfig.from_pretrained(checkpoint, ignore_mismatched_sizes=True)\n",
        "\n",
        "# Create an audio classification model 'model_pred' using the 'ASTForAudioClassification' class and load it from the specified checkpoint directory..\n",
        "model_pred = AutoModelForAudioClassification.from_pretrained(checkpoint, config=model_config_pred, ignore_mismatched_sizes=True)\n"
      ],
      "metadata": {
        "id": "YGqiaPgPFPZi"
      },
      "id": "YGqiaPgPFPZi",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess the test dataset by applying the 'preprocess_function_pred_chunks' function to each example.\n",
        "\n",
        "test_dataset_encoded_pred_chunks = test_dataset.map(lambda x: preprocess_function_pred_chunks(x, model_pred),\n",
        "                                                              remove_columns=[\"audio\"],\n",
        "                                                              batched=False,\n",
        "                                                              batch_size=1)\n",
        "\n",
        "# Convert the preprocessed test dataset to a pandas DataFrame, and select the columns 'file_name' and 'predicted_class_id'.\n",
        "# The 'file_name' column contains the file names of the audio examples, and the 'predicted_class_id' column contains the predicted class IDs.\n",
        "pred_pandas_df = test_dataset_encoded_pred_chunks.to_pandas()[['file_name', 'predicted_class_id']]\n",
        "\n",
        "# Save the selected columns to a CSV file with a name containing the value of 'N'.\n",
        "# The CSV file will be stored in the '/content/drive/MyDrive/gdsc/Kamran/models/final/' directory.\n",
        "pred_pandas_df.to_csv(f'{MY_PATH}/Kamran/models/final/final_{N}.csv', index=False)\n"
      ],
      "metadata": {
        "id": "FXG4UD6bHHaO"
      },
      "id": "FXG4UD6bHHaO",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "availableInstances": [
      {
        "_defaultOrder": 0,
        "_isFastLaunch": true,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 4,
        "name": "ml.t3.medium",
        "vcpuNum": 2
      },
      {
        "_defaultOrder": 1,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 8,
        "name": "ml.t3.large",
        "vcpuNum": 2
      },
      {
        "_defaultOrder": 2,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 16,
        "name": "ml.t3.xlarge",
        "vcpuNum": 4
      },
      {
        "_defaultOrder": 3,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 32,
        "name": "ml.t3.2xlarge",
        "vcpuNum": 8
      },
      {
        "_defaultOrder": 4,
        "_isFastLaunch": true,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 8,
        "name": "ml.m5.large",
        "vcpuNum": 2
      },
      {
        "_defaultOrder": 5,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 16,
        "name": "ml.m5.xlarge",
        "vcpuNum": 4
      },
      {
        "_defaultOrder": 6,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 32,
        "name": "ml.m5.2xlarge",
        "vcpuNum": 8
      },
      {
        "_defaultOrder": 7,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 64,
        "name": "ml.m5.4xlarge",
        "vcpuNum": 16
      },
      {
        "_defaultOrder": 8,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 128,
        "name": "ml.m5.8xlarge",
        "vcpuNum": 32
      },
      {
        "_defaultOrder": 9,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 192,
        "name": "ml.m5.12xlarge",
        "vcpuNum": 48
      },
      {
        "_defaultOrder": 10,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 256,
        "name": "ml.m5.16xlarge",
        "vcpuNum": 64
      },
      {
        "_defaultOrder": 11,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 384,
        "name": "ml.m5.24xlarge",
        "vcpuNum": 96
      },
      {
        "_defaultOrder": 12,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 8,
        "name": "ml.m5d.large",
        "vcpuNum": 2
      },
      {
        "_defaultOrder": 13,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 16,
        "name": "ml.m5d.xlarge",
        "vcpuNum": 4
      },
      {
        "_defaultOrder": 14,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 32,
        "name": "ml.m5d.2xlarge",
        "vcpuNum": 8
      },
      {
        "_defaultOrder": 15,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 64,
        "name": "ml.m5d.4xlarge",
        "vcpuNum": 16
      },
      {
        "_defaultOrder": 16,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 128,
        "name": "ml.m5d.8xlarge",
        "vcpuNum": 32
      },
      {
        "_defaultOrder": 17,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 192,
        "name": "ml.m5d.12xlarge",
        "vcpuNum": 48
      },
      {
        "_defaultOrder": 18,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 256,
        "name": "ml.m5d.16xlarge",
        "vcpuNum": 64
      },
      {
        "_defaultOrder": 19,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 384,
        "name": "ml.m5d.24xlarge",
        "vcpuNum": 96
      },
      {
        "_defaultOrder": 20,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": true,
        "memoryGiB": 0,
        "name": "ml.geospatial.interactive",
        "supportedImageNames": [
          "sagemaker-geospatial-v1-0"
        ],
        "vcpuNum": 0
      },
      {
        "_defaultOrder": 21,
        "_isFastLaunch": true,
        "category": "Compute optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 4,
        "name": "ml.c5.large",
        "vcpuNum": 2
      },
      {
        "_defaultOrder": 22,
        "_isFastLaunch": false,
        "category": "Compute optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 8,
        "name": "ml.c5.xlarge",
        "vcpuNum": 4
      },
      {
        "_defaultOrder": 23,
        "_isFastLaunch": false,
        "category": "Compute optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 16,
        "name": "ml.c5.2xlarge",
        "vcpuNum": 8
      },
      {
        "_defaultOrder": 24,
        "_isFastLaunch": false,
        "category": "Compute optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 32,
        "name": "ml.c5.4xlarge",
        "vcpuNum": 16
      },
      {
        "_defaultOrder": 25,
        "_isFastLaunch": false,
        "category": "Compute optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 72,
        "name": "ml.c5.9xlarge",
        "vcpuNum": 36
      },
      {
        "_defaultOrder": 26,
        "_isFastLaunch": false,
        "category": "Compute optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 96,
        "name": "ml.c5.12xlarge",
        "vcpuNum": 48
      },
      {
        "_defaultOrder": 27,
        "_isFastLaunch": false,
        "category": "Compute optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 144,
        "name": "ml.c5.18xlarge",
        "vcpuNum": 72
      },
      {
        "_defaultOrder": 28,
        "_isFastLaunch": false,
        "category": "Compute optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 192,
        "name": "ml.c5.24xlarge",
        "vcpuNum": 96
      },
      {
        "_defaultOrder": 29,
        "_isFastLaunch": true,
        "category": "Accelerated computing",
        "gpuNum": 1,
        "hideHardwareSpecs": false,
        "memoryGiB": 16,
        "name": "ml.g4dn.xlarge",
        "vcpuNum": 4
      },
      {
        "_defaultOrder": 30,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 1,
        "hideHardwareSpecs": false,
        "memoryGiB": 32,
        "name": "ml.g4dn.2xlarge",
        "vcpuNum": 8
      },
      {
        "_defaultOrder": 31,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 1,
        "hideHardwareSpecs": false,
        "memoryGiB": 64,
        "name": "ml.g4dn.4xlarge",
        "vcpuNum": 16
      },
      {
        "_defaultOrder": 32,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 1,
        "hideHardwareSpecs": false,
        "memoryGiB": 128,
        "name": "ml.g4dn.8xlarge",
        "vcpuNum": 32
      },
      {
        "_defaultOrder": 33,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 4,
        "hideHardwareSpecs": false,
        "memoryGiB": 192,
        "name": "ml.g4dn.12xlarge",
        "vcpuNum": 48
      },
      {
        "_defaultOrder": 34,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 1,
        "hideHardwareSpecs": false,
        "memoryGiB": 256,
        "name": "ml.g4dn.16xlarge",
        "vcpuNum": 64
      },
      {
        "_defaultOrder": 35,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 1,
        "hideHardwareSpecs": false,
        "memoryGiB": 61,
        "name": "ml.p3.2xlarge",
        "vcpuNum": 8
      },
      {
        "_defaultOrder": 36,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 4,
        "hideHardwareSpecs": false,
        "memoryGiB": 244,
        "name": "ml.p3.8xlarge",
        "vcpuNum": 32
      },
      {
        "_defaultOrder": 37,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 8,
        "hideHardwareSpecs": false,
        "memoryGiB": 488,
        "name": "ml.p3.16xlarge",
        "vcpuNum": 64
      },
      {
        "_defaultOrder": 38,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 8,
        "hideHardwareSpecs": false,
        "memoryGiB": 768,
        "name": "ml.p3dn.24xlarge",
        "vcpuNum": 96
      },
      {
        "_defaultOrder": 39,
        "_isFastLaunch": false,
        "category": "Memory Optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 16,
        "name": "ml.r5.large",
        "vcpuNum": 2
      },
      {
        "_defaultOrder": 40,
        "_isFastLaunch": false,
        "category": "Memory Optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 32,
        "name": "ml.r5.xlarge",
        "vcpuNum": 4
      },
      {
        "_defaultOrder": 41,
        "_isFastLaunch": false,
        "category": "Memory Optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 64,
        "name": "ml.r5.2xlarge",
        "vcpuNum": 8
      },
      {
        "_defaultOrder": 42,
        "_isFastLaunch": false,
        "category": "Memory Optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 128,
        "name": "ml.r5.4xlarge",
        "vcpuNum": 16
      },
      {
        "_defaultOrder": 43,
        "_isFastLaunch": false,
        "category": "Memory Optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 256,
        "name": "ml.r5.8xlarge",
        "vcpuNum": 32
      },
      {
        "_defaultOrder": 44,
        "_isFastLaunch": false,
        "category": "Memory Optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 384,
        "name": "ml.r5.12xlarge",
        "vcpuNum": 48
      },
      {
        "_defaultOrder": 45,
        "_isFastLaunch": false,
        "category": "Memory Optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 512,
        "name": "ml.r5.16xlarge",
        "vcpuNum": 64
      },
      {
        "_defaultOrder": 46,
        "_isFastLaunch": false,
        "category": "Memory Optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 768,
        "name": "ml.r5.24xlarge",
        "vcpuNum": 96
      },
      {
        "_defaultOrder": 47,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 1,
        "hideHardwareSpecs": false,
        "memoryGiB": 16,
        "name": "ml.g5.xlarge",
        "vcpuNum": 4
      },
      {
        "_defaultOrder": 48,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 1,
        "hideHardwareSpecs": false,
        "memoryGiB": 32,
        "name": "ml.g5.2xlarge",
        "vcpuNum": 8
      },
      {
        "_defaultOrder": 49,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 1,
        "hideHardwareSpecs": false,
        "memoryGiB": 64,
        "name": "ml.g5.4xlarge",
        "vcpuNum": 16
      },
      {
        "_defaultOrder": 50,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 1,
        "hideHardwareSpecs": false,
        "memoryGiB": 128,
        "name": "ml.g5.8xlarge",
        "vcpuNum": 32
      },
      {
        "_defaultOrder": 51,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 1,
        "hideHardwareSpecs": false,
        "memoryGiB": 256,
        "name": "ml.g5.16xlarge",
        "vcpuNum": 64
      },
      {
        "_defaultOrder": 52,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 4,
        "hideHardwareSpecs": false,
        "memoryGiB": 192,
        "name": "ml.g5.12xlarge",
        "vcpuNum": 48
      },
      {
        "_defaultOrder": 53,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 4,
        "hideHardwareSpecs": false,
        "memoryGiB": 384,
        "name": "ml.g5.24xlarge",
        "vcpuNum": 96
      },
      {
        "_defaultOrder": 54,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 8,
        "hideHardwareSpecs": false,
        "memoryGiB": 768,
        "name": "ml.g5.48xlarge",
        "vcpuNum": 192
      },
      {
        "_defaultOrder": 55,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 8,
        "hideHardwareSpecs": false,
        "memoryGiB": 1152,
        "name": "ml.p4d.24xlarge",
        "vcpuNum": 96
      },
      {
        "_defaultOrder": 56,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 8,
        "hideHardwareSpecs": false,
        "memoryGiB": 1152,
        "name": "ml.p4de.24xlarge",
        "vcpuNum": 96
      }
    ],
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "machine_shape": "hm"
    },
    "instance_type": "ml.g4dn.xlarge",
    "kernelspec": {
      "display_name": "GDSC (custom-gdsc/1)",
      "language": "python",
      "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:026922381374:image-version/custom-gdsc/1"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    },
    "lcc_arn": "arn:aws:sagemaker:us-east-1:026922381374:studio-lifecycle-config/clean-trash",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "cceb7a9586c54edea23246dff1932b8a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1269afec90064d03bffc7cf1b40dda18",
              "IPY_MODEL_14875bb04d29487ea48e71ba56673faf",
              "IPY_MODEL_2644de80f7ac4e4c98b80dc95479698e"
            ],
            "layout": "IPY_MODEL_055760bd2f864fcfb2a3eccefabcbdf1"
          }
        },
        "1269afec90064d03bffc7cf1b40dda18": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_40eceae3e88b4d36afa05bfb775f3f5b",
            "placeholder": "",
            "style": "IPY_MODEL_e4fdf479f078460bba0eacf0ce7d2bc5",
            "value": "Resolving data files: 100%"
          }
        },
        "14875bb04d29487ea48e71ba56673faf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bd699fe22f934748a5acdde2ab1bf052",
            "max": 580,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_855457e393db4775ab34e43f81ff44b2",
            "value": 580
          }
        },
        "2644de80f7ac4e4c98b80dc95479698e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e2f2e11be2e64b1d9246fe5af5ffafab",
            "placeholder": "",
            "style": "IPY_MODEL_12f74ed3c7554df3b7e142bb58425f59",
            "value": " 580/580 [00:00&lt;00:00, 3282.92it/s]"
          }
        },
        "055760bd2f864fcfb2a3eccefabcbdf1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "40eceae3e88b4d36afa05bfb775f3f5b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e4fdf479f078460bba0eacf0ce7d2bc5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bd699fe22f934748a5acdde2ab1bf052": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "855457e393db4775ab34e43f81ff44b2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e2f2e11be2e64b1d9246fe5af5ffafab": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "12f74ed3c7554df3b7e142bb58425f59": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a7011dac3cb6477596042bcb0159ec4d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1559ac70674c45a3bd9dcf2dc75921a4",
              "IPY_MODEL_7f64587706cd4bffa582eeb5b883d494",
              "IPY_MODEL_2b5e606a3f5c4598af65ffdb5446daf1"
            ],
            "layout": "IPY_MODEL_87b05c966f6c49f19c800be329553686"
          }
        },
        "1559ac70674c45a3bd9dcf2dc75921a4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c3177d4decda4a16ba4a19be2b3c0f37",
            "placeholder": "",
            "style": "IPY_MODEL_c20183e0dc394e049c5a3c68f2f19256",
            "value": "100%"
          }
        },
        "7f64587706cd4bffa582eeb5b883d494": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5b93f4bb24a34b1aaa54782f03116fd9",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8612c99f887b4e5da40c2bfa3da2bdd4",
            "value": 1
          }
        },
        "2b5e606a3f5c4598af65ffdb5446daf1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7778b27815e8496cac084ee06489300d",
            "placeholder": "",
            "style": "IPY_MODEL_2e9758759ade45a8932c7b40e2952940",
            "value": " 1/1 [00:00&lt;00:00, 57.28it/s]"
          }
        },
        "87b05c966f6c49f19c800be329553686": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c3177d4decda4a16ba4a19be2b3c0f37": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c20183e0dc394e049c5a3c68f2f19256": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5b93f4bb24a34b1aaa54782f03116fd9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8612c99f887b4e5da40c2bfa3da2bdd4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7778b27815e8496cac084ee06489300d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2e9758759ade45a8932c7b40e2952940": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9e7a5810af29401c8480d38e719d616a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_cd0515231c9648f1b660f749fbd29a9d",
              "IPY_MODEL_62a08b0f68cd4086bc11772435e241a2",
              "IPY_MODEL_b15496c9085a4b689b18077b5fa5b17a"
            ],
            "layout": "IPY_MODEL_6db9e0697f944606b1db70dcdb2fc837"
          }
        },
        "cd0515231c9648f1b660f749fbd29a9d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9cac9917bc0f4ce19f01d1fb6aad735c",
            "placeholder": "",
            "style": "IPY_MODEL_cfdda946c5924a509531d4910fb8b4a3",
            "value": "Resolving data files: 100%"
          }
        },
        "62a08b0f68cd4086bc11772435e241a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_602a038ab8264406aab070d22b72f424",
            "max": 557,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e9046a3c8e94468d93fbc6255b33f9b0",
            "value": 557
          }
        },
        "b15496c9085a4b689b18077b5fa5b17a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6bcd0c51353f440f9820cfaab3af85a4",
            "placeholder": "",
            "style": "IPY_MODEL_c778835e15ff4b68a1fb2501582c6d3d",
            "value": " 557/557 [00:00&lt;00:00, 2826.91it/s]"
          }
        },
        "6db9e0697f944606b1db70dcdb2fc837": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9cac9917bc0f4ce19f01d1fb6aad735c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cfdda946c5924a509531d4910fb8b4a3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "602a038ab8264406aab070d22b72f424": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e9046a3c8e94468d93fbc6255b33f9b0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6bcd0c51353f440f9820cfaab3af85a4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c778835e15ff4b68a1fb2501582c6d3d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5e42a61b3ec743fda502cf7df9960819": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c7c26944a14244e9a5f7477c55217e56",
              "IPY_MODEL_ad565dc2a7a34466a9253e1e4d1bf85b",
              "IPY_MODEL_cf57ccbdc65643368f40d580c853a232"
            ],
            "layout": "IPY_MODEL_3efc2571022c4798b1766bac5ecb86d9"
          }
        },
        "c7c26944a14244e9a5f7477c55217e56": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a4533dfdf1954a9d940016adab5c316e",
            "placeholder": "",
            "style": "IPY_MODEL_9eee968399b9454d9327c052b35318dc",
            "value": "100%"
          }
        },
        "ad565dc2a7a34466a9253e1e4d1bf85b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fdca30d835fd4215b527cb0705643dd4",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0969db5fec97459c9e2e931f48a5864d",
            "value": 1
          }
        },
        "cf57ccbdc65643368f40d580c853a232": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ac75da7f3c0b41fda42ba5c92b8b2cac",
            "placeholder": "",
            "style": "IPY_MODEL_52060d9c48914dff80dff532685f0317",
            "value": " 1/1 [00:00&lt;00:00, 51.76it/s]"
          }
        },
        "3efc2571022c4798b1766bac5ecb86d9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a4533dfdf1954a9d940016adab5c316e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9eee968399b9454d9327c052b35318dc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fdca30d835fd4215b527cb0705643dd4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0969db5fec97459c9e2e931f48a5864d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ac75da7f3c0b41fda42ba5c92b8b2cac": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "52060d9c48914dff80dff532685f0317": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}